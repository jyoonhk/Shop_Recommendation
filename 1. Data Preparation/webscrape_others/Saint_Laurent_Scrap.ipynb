{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ae04ee68dffee34b20423ee6009830f7967d7857e9eaf0705903c4e3b1f453d9",
   "display_name": "Python 3.8.5 64-bit ('FTDS': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "#Define links in website to scrape, split by clothing type\n",
    "# This assumes each link is in the same format and can be scraped using the same variables defined later.\n",
    "saint_laurent = {\n",
    "    'Mens Shirts': 'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/shirts',\n",
    "    'Mens Knitwear': 'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/knitwear',\n",
    "    'Mens Denim':'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/denim',\n",
    "    'Mens T-shirt_and_Sweatshirts':'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/t-shirts-and-sweatshirts',\n",
    "    'Mens Leather_and_Fur': 'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/leather-and-fur',\n",
    "    'Mens Outerwear':'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/leather-and-fur',\n",
    "    'Mens Jackets_and_Pants': 'https://www.ysl.com/en-hk/shop-men/ready-to-wear/by-category/jackets-and-pants',\n",
    "    'Mens Jewelry':'https://www.ysl.com/en-hk/search?cgid=view-all-jewellry-men',\n",
    "    'Mens Shoes':'https://www.ysl.com/en-hk/shop-men/shoes/view-all',\n",
    "    'Mens Hats':'https://www.ysl.com/en-hk/shop-men/accessories/hats',\n",
    "    'Mens Sunglasses':'https://www.ysl.com/en-hk/shop-men/sunglasses',\n",
    "    'Mens Bags':'https://www.ysl.com/en-hk/shop-men/bags/view-all',\n",
    "\n",
    "    'Womens T-Shirts_and_Sweatshirts': 'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/t-shirts-and-sweatshirts',\n",
    "    'Womens Knitwear': 'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/knitwear',\n",
    "    'Womens Leather_and_Fur':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/leather-and-fur',\n",
    "    'Womens Jackets':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/jackets',\n",
    "    'Womens Outerwear': 'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/outerwear',\n",
    "    'Womens Trousers_and_Shorts':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/trousers-and-shorts',\n",
    "    'Womens Shirts_and_Blouses':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/shirts-and-blouses',\n",
    "    'Womens Denim': 'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/denim',\n",
    "    'Womens Lingerie':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/lingerie',\n",
    "    'Womens Dresses_and_Skirts':'https://www.ysl.com/en-hk/shop-women/ready-to-wear/by-category/dresses-and-skirts',\n",
    "    'Womens Shoes':'https://www.ysl.com/en-hk/shop-women/shoes/view-all',\n",
    "    'Womens Handbags':'https://www.ysl.com/en-hk/shop-women/handbags/view-all',\n",
    "    'Womens Hats_and_Gloves':'https://www.ysl.com/en-hk/shop-women/accessories/hats-and-gloves',\n",
    "    'Womens Sunglasses':'https://www.ysl.com/en-hk/shop-women/sunglasses',\n",
    "    'Womens Jewellry':'https://www.ysl.com/en-hk/shop-women/jewellery/view-all'\n",
    "    }\n",
    "\n",
    "#Set destination folder to save subfolder of images\n",
    "#Additional sub folders for each clothing category will be automatically created to save images\n",
    "folder = 'C:/Users/Yoon Hwan Kim/Documents/Shop_Recommendation_Project/Scrap_1'\n",
    "\n",
    "#Create empty df to save all information to\n",
    "df = pd.DataFrame(columns = ['ID', 'Clothing Category','Brand', 'Price', 'Image URL'])\n",
    "\n",
    "#Loop through links in the dictionary \n",
    "for category in saint_laurent.keys():\n",
    "    #Create a new folder for each clothing category, and change to this folder\n",
    "    try:\n",
    "        os.mkdir(os.path.join(folder, str(category)))\n",
    "        os.chdir(os.path.join(folder, str(category)))\n",
    "    except:\n",
    "        pass\n",
    "    #Use Selenium to begin scrape\n",
    "    url = saint_laurent[category]\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    subhtml = driver.page_source\n",
    "    soup = bs(subhtml, \"html.parser\")\n",
    "    \n",
    "    #Scroll down and rescrape, stop when last_height = new_height, since no new images\n",
    "    while True:\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        try:\n",
    "            element=driver.find_element_by_css_selector('button.c-loadmore__btn.c-button--animation')\n",
    "            ActionChains(driver).click(element).perform()\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            subhtml = driver.page_source\n",
    "            soup = bs(subhtml, \"html.parser\")\n",
    "            break\n",
    "        \n",
    "        last_height = new_height\n",
    "        subhtml = driver.page_source\n",
    "        soup = bs(subhtml, \"html.parser\")\n",
    "\n",
    "    # except TimeoutException\n",
    "\n",
    "    #Begin scrape once we have scrolled to the bottom of the page\n",
    "    id_no = len(df)\n",
    "\n",
    "    # Define the variables 'brand', 'product_category', 'price' and 'url / src' locations in the webpage\n",
    "    # 'brand' = defined by shop to categorise groups of similar products, e.g. \"Mens Sports Performance Training\", 'HeatTech catalogue', 'Dry-Fit', 'ColdTech', 'Nike Yoga' etc\n",
    "    # 'product category' = defined by shop for product types, e.g. 'long sleeved shirt', 'Men's Perfomance shorts', 'Women's Running Tights', 'Men's Basketball Hoodie' \n",
    "    # 'src' = url for link to the picture/image. This is used by the requests.get() function to download the image, check there is a 'http:' at the start of the string\n",
    "    \n",
    "    for product in soup.find_all(class_=\"c-product__inner\"): # change 'class_ = ...' to how the each image / product is split in the website \n",
    "        try:\n",
    "            brand = product.find(class_='c-product__name').text\n",
    "            price = product.find(class_='c-price__value--current').text.strip()\n",
    "            src = product.find(class_='c-product__image')['src']\n",
    "\n",
    "            # Create a dummy dataframe and append to main df\n",
    "            df1 = pd.DataFrame({'ID': int(id_no), \n",
    "                'Clothing Category':str(category),\n",
    "                'Brand':[brand],\n",
    "                'Price':[price], \n",
    "                'Image URL':[src]\n",
    "                })\n",
    "            df = df.append(df1)\n",
    "            \n",
    "            #Save image into folder\n",
    "            image_name = str(id_no) + str(\" \") + str(brand) + str(\".jpg\")\n",
    "            image_link = src\n",
    "            with open(str(image_name),'wb') as f:\n",
    "                im = requests.get(image_link)\n",
    "                f.write(im.content)\n",
    "            id_no+=1\n",
    "        #some will fail... this is expected\n",
    "        except:\n",
    "            print('Failed')\n",
    "            id_no+=1\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(55, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(folder)\n",
    "df.to_csv('Saint_Laurent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}